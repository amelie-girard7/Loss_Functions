The first is called the 'negative log-likelihood' (NLL) and is the universal loss function used to train classification models (including NLG models).
The NLL is the practical way to achieve 'maximum likelihood estimation' (MLE), because the likelihood and the log-likelihood are maximised by the same argument.
In addition, many readers struggle to realise that it's identical to another loss function, the so-called 'cross entropy', simply because the latter is always introduced for the two-class (i.e., binary) case only, and with different notations. But they are exactly the same.
Curiously, given that the ground truth is always 0 or 1, the NLL is also an instance of the Kullback-Leibler divergence, a popular way to measure the difference between two distributions.
I have managed to find an article that talks about all this:
http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html



The other topic is called optimal transport (OT). It is a classic optimisation framework that is able to find optimal assignments (i.e., matching coefficients) between the elements of two weighted sets, A = {(a_1, w_1),...(a_N,w_N)} and B ={(b_1, w_1),...(b_M,w_M)} (note that they can be of different size, N and M). Optimal transport has also found some use in NLG as a loss function alternative to the NLL/MLE: it compares the predictions made by a model with the ground truth, and their OT distance is used as a loss function for training. This paper is a good example:
https://arxiv.org/abs/1901.06283
 
It would be great if you could dissect these two references and give  a brief presentation whenever you are ready (no rush on our end).
