{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Foundations of Negative Log-Likelihood (NLL)\n",
    "\n",
    "### What is Negative Log-Likelihood (NLL)?\n",
    "\n",
    "NLL is a loss function commonly used in machine learning and statistics. It is essentially the negative logarithm of the likelihood function. The likelihood function measures how well your model explains the observed data. In the context of NLG models, the likelihood function often represents the probability of generating a particular sequence of words given a certain context.\n",
    "\n",
    "### Why is NLL used as a loss function?\n",
    "\n",
    "NLL is closely related to Maximum Likelihood Estimation (MLE). While MLE aims to maximize the likelihood function, minimizing NLL is equivalent to maximizing the likelihood. This is particularly useful in NLG models where you want to maximize the probability of generating the most likely sequence of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Equations for NLL\n",
    "\n",
    "The NLL loss function is defined as:\n",
    "\n",
    "$[\n",
    "\\text{NLL} = -\\log L(\\theta; \\mathbf{X}) = -\\sum_{i=1}^{n} \\log p(x_i; \\theta)\n",
    "$]\n",
    "\n",
    "where $L(\\theta; \\mathbf{X})$ is the likelihood of the observed data $\\mathbf{X}$ given the model parameters $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications of NLL in NLG\n",
    "\n",
    "### Role in Sequence Generation\n",
    "\n",
    "In NLG models like GPT or LSTM-based sequence generators, NLL is used to train the model to generate sequences that are most likely to occur in the real world.\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "During training, the model parameters are updated to minimize the NLL. This ensures that the generated text is coherent and contextually relevant.\n",
    "\n",
    "### Softmax Activation\n",
    "\n",
    "In the final layer of the model, a softmax activation function is often used to convert raw scores to probabilities. The NLL is then computed based on these probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Aspects of NLL\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "SGD or its variants (like Adam) are commonly used to minimize the NLL loss function.\n",
    "\n",
    "### Efficiency\n",
    "\n",
    "NLL is computationally efficient to calculate and differentiate, making it suitable for large-scale NLG models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarifications and Misconceptions about NLL\n",
    "\n",
    "NLL is essentially a specific case of cross-entropy loss when dealing with true labels that are one-hot encoded. In the context of NLG, both terms are often used interchangeably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands on example\n",
    "Below is a Python code snippet that demonstrates the concept of Negative Log-Likelihood (NLL) in the context of a simple Natural Language Generation (NLG) task. In this example, we'll use a toy dataset and a basic model to predict the next word in a sequence.\n",
    "\n",
    "- Data Preparation: We have a toy dataset where each sequence is a list of words. The last word in each sequence is what we want to predict.\n",
    "\n",
    "- Model: We use a simple RNN model for this task. The model takes the word indices as input, embeds them into a continuous space, passes them through an RNN layer, and finally through a fully connected layer to predict the next word.\n",
    "\n",
    "- Loss Function: We use the CrossEntropyLoss, which is a generalization of NLL loss suitable for multi-class classification.\n",
    "\n",
    "- Training: We train the model using the Adam optimizer. The model parameters are updated to minimize the NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Toy dataset: Each row is a sequence, and the last element is the target word\n",
    "data = [\n",
    "    ['hello', 'how', 'are', 'you'],\n",
    "    ['I', 'am', 'fine'],\n",
    "    ['hello', 'I', 'am', 'Amelie']\n",
    "]\n",
    "word_to_idx = {'<PAD>': 0, 'hello': 1, 'how': 2, 'are': 3, 'you': 4, 'I': 5, 'am': 6, 'fine': 7, 'Amelie': 8}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# Convert words to integers\n",
    "data_idx = [[word_to_idx[w] for w in seq] for seq in data]\n",
    "\n",
    "# Prepare data for training\n",
    "X_train = [torch.tensor(seq[:-1], dtype=torch.long) for seq in data_idx]\n",
    "y_train = [torch.tensor(seq[-1], dtype=torch.long) for seq in data_idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.9169\n",
      "Epoch [20/100], Loss: 1.6993\n",
      "Epoch [30/100], Loss: 1.4279\n",
      "Epoch [40/100], Loss: 1.1112\n",
      "Epoch [50/100], Loss: 0.8632\n",
      "Epoch [60/100], Loss: 0.7106\n",
      "Epoch [70/100], Loss: 0.6101\n",
      "Epoch [80/100], Loss: 0.5385\n",
      "Epoch [90/100], Loss: 0.4850\n",
      "Epoch [100/100], Loss: 0.4435\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Simple RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 10\n",
    "hidden_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleRNN(vocab_size, embed_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()  # NLL loss is a specific case of CrossEntropyLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        x = x.view(-1, 1)\n",
    "        output = model(x)\n",
    "        \n",
    "        # Repeat y to match the batch size\n",
    "        y_repeated = y.repeat(output.shape[0])\n",
    "        \n",
    "        loss = criterion(output, y_repeated)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
